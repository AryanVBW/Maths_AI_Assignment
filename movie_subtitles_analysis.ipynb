{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Subtitles Analysis: Benford's Law and Zipf's Law\n",
    "\n",
    "This notebook analyzes the `movies_subtitles.csv` dataset to examine the applicability of two statistical patterns:\n",
    "1. **Benford's Law** - Applied to the start times of subtitles\n",
    "2. **Zipf's Law** - Applied to word frequencies in subtitle text\n",
    "**Team Name:** The Procrastinators' Club\n",
    "## Members\n",
    "1. **Naman Gupta**\n",
    "   - Email: naman.gupta@adypu.edu.in\n",
    "   - Batch: B\n",
    "2. **Vivek Wagadare**\n",
    "   - Email: vivek.wagadare@adypu.edu.in\n",
    "   - Batch: C\n",
    "3. **Devendra Mishra**\n",
    "   - Email: devendra.mishra@adypu.edu.in\n",
    "   - Batch: C\n",
    "4. **Aryan Rana**\n",
    "   - Email: aryan.rana@adypu.edu.in\n",
    "   - Batch: A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Options\n",
    "\n",
    "You can either upload the dataset directly or access it from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# Install NLTK and download required data\n",
    "!pip install nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Upload Dataset Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_upload():\n",
    "    uploaded = files.upload()  # This will prompt the user to upload a file\n",
    "    for filename in uploaded.keys():\n",
    "        print(f'Uploaded file: {filename}')\n",
    "        if filename.endswith('.csv'):\n",
    "            return pd.read_csv(io.BytesIO(uploaded[filename]))\n",
    "    raise ValueError(\"Please upload a CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Load from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "def mount_drive():\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Drive mounted successfully.\")\n",
    "    \n",
    "def load_from_drive(file_path=None):\n",
    "    if file_path is None:\n",
    "        file_path = input(\"Enter the path to the CSV file in your Google Drive (e.g., 'My Drive/movies_subtitles.csv'): \")\n",
    "    \n",
    "    full_path = f\"/content/drive/{file_path}\"\n",
    "    try:\n",
    "        return pd.read_csv(full_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found at {full_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_drive_link():\n",
    "    drive_link = input(\"Enter the Google Drive shared link for the CSV file: \")\n",
    "    file_id = None\n",
    "    \n",
    "    # Extract file ID from drive link\n",
    "    if 'drive.google.com/file/d/' in drive_link:\n",
    "        file_id = drive_link.split('drive.google.com/file/d/')[1].split('/')[0]\n",
    "    elif 'drive.google.com/open?id=' in drive_link:\n",
    "        file_id = drive_link.split('drive.google.com/open?id=')[1].split('&')[0]\n",
    "    \n",
    "    if file_id:\n",
    "        download_link = f'https://drive.google.com/uc?id={file_id}'\n",
    "        return pd.read_csv(download_link)\n",
    "    else:\n",
    "        print(\"Invalid Google Drive link format.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Data Loading Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Choose how to load the dataset:\")\n",
    "print(\"1. Upload CSV file directly\")\n",
    "print(\"2. Access from mounted Google Drive\")\n",
    "print(\"3. Use Google Drive shared link\")\n",
    "\n",
    "choice = input(\"Enter your choice (1, 2, or 3): \")\n",
    "\n",
    "if choice == '1':\n",
    "    df = load_from_upload()\n",
    "elif choice == '2':\n",
    "    mount_drive()\n",
    "    df = load_from_drive()\n",
    "elif choice == '3':\n",
    "    df = load_from_drive_link()\n",
    "else:\n",
    "    print(\"Invalid choice. Please run the cell again and choose 1, 2, or 3.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    \n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nData summary:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    display(df.isnull().sum())\n",
    "else:\n",
    "    print(\"Dataset not loaded. Please run the data loading cells again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benford's Law Analysis\n",
    "\n",
    "Benford's Law predicts that in many naturally occurring collections of numbers, the first significant digit is likely to be small. Specifically:\n",
    "- The digit '1' appears as the leading digit about 30.1% of the time\n",
    "- The digit '2' about 17.6% of the time\n",
    "- The digit '3' about 12.5% of the time\n",
    "- And so on...\n",
    "\n",
    "Formula: P(d) = log10(1 + 1/d) for d ∈ {1, 2, ..., 9}\n",
    "\n",
    "We'll analyze the `start_time` column to see if it follows Benford's Law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_digit(number):\n",
    "    # Convert to string and find first non-zero digit\n",
    "    str_num = str(number)\n",
    "    for char in str_num:\n",
    "        if char.isdigit() and char != '0':\n",
    "            return int(char)\n",
    "    return None\n",
    "\n",
    "def benford_expected_distribution():\n",
    "    return {d: math.log10(1 + 1/d) for d in range(1, 10)}\n",
    "\n",
    "def analyze_benford_law(df, column_name):\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract valid numeric data\n",
    "    numeric_data = df[column_name].dropna()\n",
    "    \n",
    "    # Extract first digits\n",
    "    first_digits = numeric_data.apply(extract_first_digit).dropna()\n",
    "    print(f\"Analyzed {len(first_digits)} valid numbers from '{column_name}' column.\")\n",
    "    \n",
    "    # Calculate observed frequencies\n",
    "    digit_counts = first_digits.value_counts().sort_index()\n",
    "    observed_distribution = (digit_counts / len(first_digits)) * 100\n",
    "    \n",
    "    # Calculate expected Benford's Law distribution\n",
    "    expected_dist = benford_expected_distribution()\n",
    "    expected_distribution = pd.Series({d: expected_dist[d] * 100 for d in range(1, 10)})\n",
    "    \n",
    "    return observed_distribution, expected_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    observed_dist, expected_dist = analyze_benford_law(df, 'start_time')\n",
    "    \n",
    "    if observed_dist is not None and expected_dist is not None:\n",
    "        # Create a DataFrame for comparison\n",
    "        benford_df = pd.DataFrame({\n",
    "            'Observed (%)': observed_dist,\n",
    "            'Expected (%)': expected_dist\n",
    "        }).round(1)\n",
    "        \n",
    "        print(\"\\nObserved vs. Expected Distribution of First Digits:\")\n",
    "        display(benford_df)\n",
    "        \n",
    "        # Calculate the difference\n",
    "        benford_df['Difference'] = (benford_df['Observed (%)'] - benford_df['Expected (%)']).round(1)\n",
    "        print(\"\\nDifference between Observed and Expected:\")\n",
    "        display(benford_df[['Difference']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Benford's Law comparison\n",
    "if df is not None and 'benford_df' in locals():\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Bar chart\n",
    "    bar_width = 0.35\n",
    "    indices = np.arange(len(benford_df.index))\n",
    "    \n",
    "    plt.bar(indices - bar_width/2, benford_df['Observed (%)'], \n",
    "            width=bar_width, label='Observed', color='skyblue', alpha=0.8)\n",
    "    plt.bar(indices + bar_width/2, benford_df['Expected (%)'], \n",
    "            width=bar_width, label='Expected (Benford\\'s Law)', color='orange', alpha=0.8)\n",
    "    \n",
    "    # Customization\n",
    "    plt.xlabel('First Digit', fontsize=14)\n",
    "    plt.ylabel('Frequency (%)', fontsize=14)\n",
    "    plt.title(\"Benford's Law: Expected vs. Observed Frequency of First Digits in start_time\", fontsize=16)\n",
    "    plt.xticks(indices, benford_df.index)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add data labels\n",
    "    for i, v in enumerate(benford_df['Observed (%)']):\n",
    "        plt.text(i - bar_width/2, v + 0.5, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "        \n",
    "    for i, v in enumerate(benford_df['Expected (%)']):\n",
    "        plt.text(i + bar_width/2, v + 0.5, f\"{v:.1f}%\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('benford_plot.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law Analysis\n",
    "\n",
    "Zipf's Law states that the frequency of any word is inversely proportional to its rank in the frequency table. If the most common word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent, etc.\n",
    "\n",
    "Formula: Frequency ≈ Constant / Rank\n",
    "\n",
    "We'll analyze the `text` column to see if word frequencies follow Zipf's Law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers and multiple spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_and_filter(text, stop_words=None):\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add custom stopwords like subtitle artifacts\n",
    "    custom_stopwords = {'sighs', 'laughs', 'coughs', 'gasps', 'inaudible', 'music'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Filter out stopwords and short words (length < 2)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def analyze_zipf_law(df, column_name, top_n=1000):\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
    "        return None\n",
    "    \n",
    "    # Preprocess all text\n",
    "    preprocessed_texts = df[column_name].apply(preprocess_text)\n",
    "    \n",
    "    # Combine all text\n",
    "    all_text = \" \".join(preprocessed_texts)\n",
    "    \n",
    "    # Tokenize and filter\n",
    "    tokens = tokenize_and_filter(all_text)\n",
    "    print(f\"Analyzed {len(tokens)} words after filtering.\")\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(tokens)\n",
    "    print(f\"Found {len(word_counts)} unique words.\")\n",
    "    \n",
    "    # Create dataframe with word frequencies and ranks\n",
    "    word_df = pd.DataFrame(word_counts.most_common(top_n), columns=['Word', 'Frequency'])\n",
    "    word_df['Rank'] = range(1, len(word_df) + 1)\n",
    "    word_df['Log_Rank'] = np.log10(word_df['Rank'])\n",
    "    word_df['Log_Frequency'] = np.log10(word_df['Frequency'])\n",
    "    \n",
    "    # Calculate expected Zipf's Law frequencies\n",
    "    c = word_df.iloc[0]['Frequency']  # Constant based on most frequent word\n",
    "    word_df['Expected_Frequency'] = c / word_df['Rank']\n",
    "    word_df['Log_Expected_Frequency'] = np.log10(word_df['Expected_Frequency'])\n",
    "    \n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    zipf_df = analyze_zipf_law(df, 'text')\n",
    "    \n",
    "    if zipf_df is not None:\n",
    "        print(\"\\nTop 10 Most Frequent Words:\")\n",
    "        display(zipf_df[['Rank', 'Word', 'Frequency']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'zipf_df' in locals():\n",
    "    # Create pie chart for top 10 words\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top10_df = zipf_df.head(10)\n",
    "    plt.pie(top10_df['Frequency'], labels=[f\"{w} ({f:,})\" for w, f in zip(top10_df['Word'], top10_df['Frequency'])],\n",
    "            autopct='%1.1f%%', shadow=True, startangle=140, explode=[0.05]*10)\n",
    "    plt.axis('equal')\n",
    "    plt.title('Frequency Share of Top 10 Words', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'zipf_df' in locals():\n",
    "    # Zipf's Law visualization - log-log plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Scatter plot of actual data\n",
    "    plt.scatter(zipf_df['Log_Rank'], zipf_df['Log_Frequency'], \n",
    "                alpha=0.6, color='blue', label='Observed')\n",
    "    \n",
    "    # Line plot of expected Zipf distribution\n",
    "    plt.plot(zipf_df['Log_Rank'], zipf_df['Log_Expected_Frequency'], \n",
    "             color='red', linestyle='-', linewidth=2, label=\"Expected (Zipf's Law)\")\n",
    "    \n",
    "    # Linear regression to find slope\n",
    "    from scipy import stats\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        zipf_df['Log_Rank'], zipf_df['Log_Frequency'])\n",
    "    \n",
    "    # Add regression line\n",
    "    plt.plot(zipf_df['Log_Rank'], intercept + slope * zipf_df['Log_Rank'], \n",
    "             color='green', linestyle='--', linewidth=2, \n",
    "             label=f'Fitted Line (slope={slope:.2f})')\n",
    "    \n",
    "    # Annotate top words\n",
    "    for i in range(min(10, len(zipf_df))):\n",
    "        plt.annotate(zipf_df.iloc[i]['Word'], \n",
    "                     (zipf_df.iloc[i]['Log_Rank'], zipf_df.iloc[i]['Log_Frequency']),\n",
    "                     xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Log(Rank)', fontsize=14)\n",
    "    plt.ylabel('Log(Frequency)', fontsize=14)\n",
    "    plt.title(\"Zipf's Law: Word Frequency vs. Rank (Log-Log Scale)\", fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add annotation about Zipf's Law\n",
    "    zipf_note = \"Zipf's Law predicts: Frequency ∝ 1/Rank\\nIdeal slope = -1.0\"\n",
    "    plt.annotate(zipf_note, xy=(0.02, 0.02), xycoords='axes fraction', \n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", alpha=0.3),\n",
    "                 fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('zipf_plot.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "**Benford's Law Analysis**\n",
    "- We examined the distribution of leading digits in the `start_time` column\n",
    "- The observed distribution showed significant deviation from Benford's Law expectations\n",
    "- Digit 1 appears less frequently than expected (~20.1% vs. expected 30.1%)\n",
    "- Digits 2-5 are overrepresented compared to their theoretical frequencies\n",
    "- This suggests that subtitle timings don't follow the naturally occurring logarithmic distribution that Benford's Law typically describes\n",
    "- This deviation might be due to the constrained nature of subtitle timing in movies\n",
    "\n",
    "**Zipf's Law Analysis**\n",
    "- We analyzed word frequencies in the `text` column after preprocessing and filtering\n",
    "- The word distribution follows Zipf's Law pattern quite closely\n",
    "- The log-log plot shows a strong linear relationship between word rank and frequency\n",
    "- The slope of this relationship is close to the theoretical value of -1\n",
    "- This confirms that even in movie subtitles, natural language follows Zipf's characteristic distribution where few words (like \"dont\", \"im\", \"know\") dominate, and frequency drops rapidly with rank\n",
    "\n",
    "Thank you "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "movie_subtitles_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
